# OmniHuman v1.5 Lipsync Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add OmniHuman v1.5 as a second lipsync model (alongside Hedra) in Avatar Studio, selected via SegmentedTabs at the Generate step.

**Architecture:** Thin adapter pattern — new `omnihuman.ts` service file, branch in existing `/api/avatars/lipsync` endpoint based on `model` body param, store state for model selection, SegmentedTabs UI. Hedra remains default.

**Tech Stack:** FAL.ai client (`fal.subscribe` + `fal.storage.upload`), Express, Zustand, React SegmentedTabs component.

**Design doc:** `docs/plans/2026-02-26-omnihuman-lipsync-design.md`

---

### Task 1: Create OmniHuman service

**Files:**
- Create: `src/server/services/omnihuman.ts`

**Step 1: Create the service file**

```typescript
import fs from 'node:fs/promises'
import path from 'node:path'
import { fal } from '@fal-ai/client'
import { ensureFalConfig } from './falConfig.js'
import {
  isMockProvidersEnabled,
  makeMockId,
  makeMockMp4DataUrl,
  recordMockProviderSuccess,
  runWithRetries,
} from './providerRuntime.js'

const OMNIHUMAN_MODEL = 'fal-ai/bytedance/omnihuman/v1.5'

export interface OmniHumanOptions {
  imageUrl: string
  audioUrl: string
  resolution?: '720p' | '1080p'
  aspectRatio?: '16:9' | '9:16' | '1:1'
}

export interface OmniHumanResult {
  videoUrl: string
  requestId: string
}

async function uploadToFalStorage(filePath: string): Promise<string> {
  const buffer = await fs.readFile(filePath)
  const ext = path.extname(filePath).toLowerCase()
  const mimeMap: Record<string, string> = {
    '.png': 'image/png',
    '.jpg': 'image/jpeg',
    '.jpeg': 'image/jpeg',
    '.webp': 'image/webp',
    '.mp3': 'audio/mpeg',
    '.wav': 'audio/wav',
    '.m4a': 'audio/mp4',
    '.aac': 'audio/aac',
    '.ogg': 'audio/ogg',
  }
  const mime = mimeMap[ext] || 'application/octet-stream'
  const blob = new Blob([buffer], { type: mime })
  return fal.storage.upload(blob)
}

export async function getAudioDuration(audioPath: string): Promise<number> {
  const { spawn } = await import('node:child_process')
  return new Promise((resolve, reject) => {
    const proc = spawn('ffprobe', [
      '-i', audioPath,
      '-show_entries', 'format=duration',
      '-v', 'quiet',
      '-of', 'csv=p=0',
    ])
    let stdout = ''
    proc.stdout.on('data', (chunk: Buffer) => { stdout += chunk.toString() })
    proc.on('close', (code) => {
      if (code === 0) resolve(parseFloat(stdout.trim()) || 0)
      else reject(new Error(`ffprobe failed with code ${code}`))
    })
    proc.on('error', () => resolve(0))
  })
}

export async function createOmniHumanVideo(
  imagePath: string,
  audioPath: string,
  options: Omit<OmniHumanOptions, 'imageUrl' | 'audioUrl'> = {},
): Promise<OmniHumanResult> {
  if (isMockProvidersEnabled()) {
    await recordMockProviderSuccess({
      pipeline: 'avatars.lipsync.provider',
      provider: 'omnihuman',
      metadata: { resolution: options.resolution || '1080p', aspectRatio: options.aspectRatio || '9:16' },
    })
    return {
      videoUrl: makeMockMp4DataUrl(),
      requestId: makeMockId('omnihuman'),
    }
  }

  ensureFalConfig()

  // Detect audio duration for resolution auto-fallback
  let resolution = options.resolution || '1080p'
  const duration = await getAudioDuration(audioPath)
  if (duration > 60) {
    throw new Error(`Audio too long for OmniHuman (${Math.round(duration)}s). Maximum is 60s at 720p.`)
  }
  if (duration > 30 && resolution === '1080p') {
    console.log(`[OmniHuman] Audio ${Math.round(duration)}s > 30s limit for 1080p, falling back to 720p`)
    resolution = '720p'
  }

  console.log(`[OmniHuman] Uploading image and audio to FAL storage...`)
  const [imageUrl, audioUrl] = await Promise.all([
    uploadToFalStorage(imagePath),
    uploadToFalStorage(audioPath),
  ])
  console.log(`[OmniHuman] Files uploaded. Generating video (${resolution})...`)

  const result = await runWithRetries(
    () =>
      fal.subscribe(OMNIHUMAN_MODEL, {
        input: {
          image_url: imageUrl,
          audio_url: audioUrl,
          resolution,
          aspect_ratio: options.aspectRatio || '9:16',
          turbo_mode: false,
        },
        logs: true,
        onQueueUpdate: (update) => {
          if (update.status === 'IN_PROGRESS' && update.logs) {
            update.logs.forEach((log) => console.log(`[OmniHuman] ${log.message}`))
          }
        },
      }),
    {
      pipeline: 'avatars.lipsync.provider',
      provider: 'omnihuman',
      metadata: { resolution, audioDuration: Math.round(duration) },
    },
  )

  const videoUrl = result.data?.video?.url
  if (!videoUrl) {
    throw new Error('No video generated by OmniHuman')
  }

  console.log(`[OmniHuman] Video ready: ${videoUrl}`)
  return { videoUrl, requestId: result.requestId }
}
```

Notes:
- `uploadToFalStorage()` reads local file → Blob → `fal.storage.upload()` → CDN URL
- `getAudioDuration()` uses ffprobe (already available via system/homebrew)
- Resolution auto-fallback: 1080p→720p if audio >30s, error if >60s
- Response schema: `result.data.video.url` (OmniHuman returns `video` not `images`)
- Same mock/retry/telemetry patterns as hedra.ts

**Step 2: Verify file compiles**

Run: `npx tsc --noEmit`

**Step 3: Commit**

```bash
git add src/server/services/omnihuman.ts
git commit -m "feat(avatar): add OmniHuman v1.5 lipsync service"
```

---

### Task 2: Branch lipsync endpoint on model param

**Files:**
- Modify: `src/server/routes/avatars.ts:768-875` (lipsync endpoint)

**Step 1: Add import**

At the top of avatars.ts, add:
```typescript
import { createOmniHumanVideo } from '../services/omnihuman.js'
```

Near existing hedra import:
```typescript
import { createHedraVideo, downloadHedraVideo } from '../services/hedra.js'
```

**Step 2: Modify lipsync handler**

At line 774, destructure `model` from request body:
```typescript
const { imageUrl, audioUrl, model } = req.body
```

Replace the Hedra-specific block (lines 846-863) with model branching:

```typescript
let videoUrl: string
let generationId: string
let outputPath: string

const lipsyncJobId = uuidv4()
const lipsyncLayout = createJobOutputDir(outputsDir, 'avatars', 'lipsync', lipsyncJobId)
await fs.mkdir(lipsyncLayout.outputDir, { recursive: true })
outputPath = path.join(lipsyncLayout.outputDir, buildJobOutputFileName('lipsync', lipsyncJobId, 'mp4'))

if (model === 'omnihuman') {
  console.log('[Lipsync] Creating video with OmniHuman v1.5...')
  const result = await createOmniHumanVideo(imagePathForLipsync, audioPath, { aspectRatio: '9:16' })
  videoUrl = result.videoUrl
  generationId = result.requestId
  await downloadHedraVideo(videoUrl, outputPath)  // reuse download helper — just fetches URL to file
} else {
  console.log('[Lipsync] Creating video with Hedra Character-3...')
  const result = await createHedraVideo({ imagePath: imagePathForLipsync, audioPath, aspectRatio: '9:16' })
  videoUrl = result.videoUrl
  generationId = result.generationId
  await downloadHedraVideo(videoUrl, outputPath)
}

if (req.user?.id)
  notify(req.user.id, 'lipsync_complete', 'Lipsync Video Ready', 'Your talking avatar video is ready to download')
span.success({ outputFile: path.relative(outputsDir, outputPath), generationId, model: model || 'hedra' })

sendSuccess(res, {
  videoUrl,
  localPath: toOutputUrl(outputsDir, outputPath),
  generationId,
})
```

**Step 3: Verify compiles**

Run: `npx tsc --noEmit`

**Step 4: Commit**

```bash
git add src/server/routes/avatars.ts
git commit -m "feat(avatar): branch lipsync endpoint for OmniHuman model"
```

---

### Task 3: Add lipsyncModel state to store

**Files:**
- Modify: `src/renderer/stores/avatarStore.ts`

**Step 1: Add type and state**

In the `AvatarState` interface (around line 334), after `lipsyncJob`:
```typescript
lipsyncModel: 'hedra' | 'omnihuman'
```

In the actions section (around line 374), near other setters:
```typescript
setLipsyncModel: (model: 'hedra' | 'omnihuman') => void
```

In the initial state (around line 448), after `generatedVideoUrl: null`:
```typescript
lipsyncModel: 'hedra',
```

In the actions implementation (around line 559), near other setters:
```typescript
setLipsyncModel: (lipsyncModel) => set({ lipsyncModel }),
```

**Step 2: Pass model to lipsync API calls**

In `createLipsync()` (line 1159-1162), add `model`:
```typescript
body: JSON.stringify({ imageUrl: avatarUrl, audioUrl: generatedAudioUrl, model: get().lipsyncModel }),
```

In `generateTalkingAvatarVideosBatch()` (line 1032-1035), add `model`:
```typescript
body: JSON.stringify({
  imageUrl: avatarUrl || undefined,
  audioUrl: item.audioUrl,
  model: get().lipsyncModel,
}),
```

**Step 3: Verify compiles**

Run: `npx tsc --noEmit`

**Step 4: Commit**

```bash
git add src/renderer/stores/avatarStore.ts
git commit -m "feat(avatar): add lipsyncModel state to avatar store"
```

---

### Task 4: Add SegmentedTabs UI + audio duration warning

**Files:**
- Modify: `src/renderer/components/avatar-studio/TalkingAvatarPage.tsx`

**Step 1: Wire store state**

In the store destructure (around line 54-100), add:
```typescript
lipsyncModel, setLipsyncModel
```

**Step 2: Add model selector items constant**

Near the top of the component (after TONE_OPTIONS or similar constants):
```typescript
const LIPSYNC_MODEL_ITEMS = [
  { id: 'hedra' as const, label: 'Hedra' },
  { id: 'omnihuman' as const, label: 'OmniHuman' },
]
```

**Step 3: Track audio duration for warning**

Add state for audio duration using the audio element:
```typescript
const [audioDuration, setAudioDuration] = useState<number>(0)
```

When audio is loaded (near the audio player), capture duration. This needs a ref or an `onLoadedMetadata` handler on the audio element.

**Step 4: Add SegmentedTabs before Generate buttons**

Before the "Have an Audio" Generate button (line 714):
```tsx
<div className="mb-3">
  <SegmentedTabs
    value={lipsyncModel}
    items={LIPSYNC_MODEL_ITEMS}
    onChange={setLipsyncModel}
    ariaLabel="Lipsync model"
    size="sm"
  />
</div>
{lipsyncModel === 'omnihuman' && audioDuration > 30 && (
  <div className="flex items-center gap-2 rounded-md bg-amber-500/10 border border-amber-500/20 px-3 py-2 mb-3">
    <AlertCircle className="w-4 h-4 text-amber-500 shrink-0" />
    <p className="text-xs text-amber-400">
      OmniHuman 1080p max 30s. Audio is {Math.round(audioDuration)}s — resolution will be lowered to 720p.
    </p>
  </div>
)}
```

Before the multi-language Generate button (line 895), add the same SegmentedTabs (shared state via store, so both modes sync).

**Step 5: Verify compiles + visual check**

Run: `npx tsc --noEmit`
Start dev server, navigate to Avatar Studio > Talking Avatar, verify tabs appear.

**Step 6: Commit**

```bash
git add src/renderer/components/avatar-studio/TalkingAvatarPage.tsx
git commit -m "feat(avatar): add OmniHuman model selector in Talking Avatar UI"
```

---

### Task 5: Update design doc + CLAUDE.md

**Files:**
- Modify: `docs/plans/2026-02-26-omnihuman-lipsync-design.md` (fix: reaction does NOT use lipsync)
- Modify: `CLAUDE.md` (add OmniHuman to model references)

**Step 1: Fix design doc**

Replace line about "Translation and Reaction flows" — reaction uses Kling (i2v), not lipsync. Only `createLipsync` and `generateTalkingAvatarVideosBatch` send model param.

**Step 2: Add to CLAUDE.md**

In the FAL Model Selection section or a new section:
```
### Lipsync Model Selection (Avatar Studio)
- `POST /api/avatars/lipsync` accepts `model?: 'hedra' | 'omnihuman'` (default: `hedra`)
- Hedra: file-based (imagePath + audioPath), custom REST API, `HEDRA_API_KEY`
- OmniHuman v1.5: URL-based (fal.storage.upload → CDN URLs), `fal-ai/bytedance/omnihuman/v1.5`, `FAL_API_KEY`
- OmniHuman resolution: 1080p default, auto-fallback to 720p for audio >30s, error >60s
```

**Step 3: Commit**

```bash
git add docs/plans/2026-02-26-omnihuman-lipsync-design.md CLAUDE.md
git commit -m "docs: update OmniHuman design doc and CLAUDE.md"
```
